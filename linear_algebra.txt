Algebra: Constructing set of objects and a set of rules to manipulate those objects, to formalize intuitive concepts.

Linear Algebra: Study of vectors and certain rules to manipulate those vectors.

In general, vectors are special objects that can be added together and multiplied by scalars to produce another object of the same kind.

From abstract mathematical viewpoint, any object that satisfies these two properties can be considered a vector.

One major idea in mathematics is the idea of "closure". What is the set of all things that can result from my proposed operations?

In the case of vectors, the question would be: What is the set of vectors that can result by starting with small set of vectors and adding them to each other and scaling them?

This results in vector space. The concept of vector space and its properties underlie much of machine learning.

Systems of Linear Equations:
Many problems can be formulated as systems of linear equations, and linear algebra gives us the tools for solving them.

In general, for a real-valued system of linear equations we obtain a either no, exactly one, or infinitely many solutions.

For a systematic approach to solving systems of linear equations, we will introduce a useful compact notation. We collect the coefficients aij into vectors and collect the vectors into matrices.

Matrices:
They can be used to compactly represent systems of linear equations, but they also represent linear functions (linear mappings).

Matrix operations:
Addition and Multiplication:
Addition is element wise, Multiplication is dot product of rows and columns. Element wise multiplication is known as hadamard product.
Associative, Distributive

Inverse and Transpose:
Inverse: A square matrix A belongs to R. Let matrix B also belongs to R have the property that AB = I = BA. B is called the inverse of A and is denoted by A^-1.
Not every matrix A posses an inverse A^-1. If inverse does exist, A is called regular/ invertible/ nonsingular, otherwise singular/ noninvertible.
When the matrix inverse exists, it is unique.

Transpose: For A belongs to R(m x n) the matrix B belongs to R(n x m) with bij = aji is called the transpose of A. We write B = A^t.

Symmetric Matrix: A matrix A belongs to R(n x n) is symmetric if A = A^t.
The sum of two symmetric matrices is always symmetric.

Multiplication by a Scalar:
If we have a scalar lambda belongs to R and a matrix A belongs to R(m x n). Then lambdaA = K, kij = lambdaaij.
Practically lambda scales each element of A.

Compact Representations of Systems of Linear Equations:
Generally, a system of linear equations can be compactly represented in the matrix form as Ax = b.

Elementary Transformations:
Key to solving a system of linear equations are elementary transformations that keep the solution set the same, but transform the equation system into a simpler form:
Exchange of two equations (rows in the matrix representing the system of equations).
Multiplication of an equation (row) with a constant.
Addition of two equations (rows).

Row-Echelon Form:
A matrix is in row-echelon form if:
All rows that contain only zeros are at the bottom of the matrix; correspondingly, all rows that contain at least one nonzero element are on top of rows that contain only zeros.
Looking at nonzero rows only, the first nonzero number from the left (also called the pivot or the leading coefficient) is always strictly to right of the pivot or the row above it.

Basic and Free Variables:
The variables correspoding to the pivots in the row-echelon form are called basic variables and the other variables are free variables.

Gaussian elimination:
Gaussian elimination is an algorithm that performs elementary transformations to bring a system of linear equations into reduced row-echelon form.

Vector Spaces:
